encoder:
  model_path: "sentence-transformers/all-MiniLM-l6-v2"
  cache_embeddings: True
  persistent_cache: True # if True, embeddings are cached in cache_dir, if False, they are cached in memory
  cache_dir: ".cache"

  model_kwargs: 
    device: "cuda"

  encode_kwargs:
    normalize_embeddings: False

retriever:
  splitting:
    chunk_size: 256
    chunk_overlap: 16
  num_docs: 8
  cache_dir: ".cache"

generator:
  llm_path: "models/phi-2.Q5_K_M.gguf"
  context_length: 2048
  temperature: 0.8
  max_tokens: 256
  gpu_layers: 50
  model_kwargs: {}
  search_kwargs: {}
  similarity_threshold: 0.8
  compress: True

dataset:
  path: "data"
  recursive: True
  show_progress: True # show progress bar when loading dataset
  use_multithreading: True
  loader_kwargs:
    mode: "single" # either "single" or "elements"
    unstructured_kwargs:
      include_metadata: True
      languages: ["en"] # language can be "auto", or a list of known languages
      chunking_strategy: "by_title" # "by_title" or null (no chunking)
prompt:
  user_name: "User"
  AI_name: "PAI"
  instruct_user: "Instruct" # usually "System", or "Instruct"
  instructions: "You are \"<<AI_NAME>>\", {user}'s personal AI assistant. Use the following to help answer {user}'s question: \"{question}\""
  pre_context: "Relevant personal notes from {user}:"
  post_context: ""
  user_query: "{question}\n\n<<AI_NAME>>:"
